var documenterSearchIndex = {"docs":
[{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Contents","page":"Reference","title":"Contents","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Index","page":"Reference","title":"Index","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [FletcherPenaltyNLPSolver]","category":"page"},{"location":"reference/#FletcherPenaltyNLPSolver.FletcherPenaltyNLP","page":"Reference","title":"FletcherPenaltyNLPSolver.FletcherPenaltyNLP","text":"We consider here the implementation of Fletcher's exact penalty method for the minimization problem:\n\nminₓ f(x) s.t. c(x) = ℓ\n\nusing Fletcher penalty function:\n\nminₓ f(x) - dot(c(x) - ℓ,ys(x)) + ρ/2 dot(c(x) - ℓ,c(x) - ℓ)\n\nwhere\n\nys(x) := argmin\\_y 0.5 ||A(x)y - g(x)||²₂ + σ (c(x) - ℓ)^T y + 0.5 δ ||²₂\n\nand denote Ys the gradient of ys(x).\n\nFletcherPenaltyNLP(:: AbstractNLPModel, :: Number, :: Function) or FletcherPenaltyNLP(:: AbstractNLPModel; σ_0 :: Real = 1.0)\n\nNotes:\n\nEvaluation of the obj, grad, objgrad functions evaluate functions from the orginial nlp.\n\nThese values are stored in fx, cx, gx.\n\nThe value of the penalty vector ys is also stored.\n\nTODO:\n\nsparse structure of the hessian?\n\nExample: fpsos  = FletcherPenaltyNLP(nlp, 0.1, _solvewithlinearoperator)\n\n\n\n\n\n","category":"type"},{"location":"reference/#FletcherPenaltyNLPSolver.feasibility_step-Union{Tuple{S}, Tuple{T}, Tuple{FletcherPenaltyNLPSolver.GNSolver, NLPModels.AbstractNLPModel, AbstractVector{T}, AbstractVector{T}, T, Union{AbstractMatrix{T}, LinearOperators.LinearOperator{T}}, T, AbstractFloat}} where {T, S}","page":"Reference","title":"FletcherPenaltyNLPSolver.feasibility_step","text":"feasibility_step(nls, x, cx, Jx)\n\nApproximately solves min ‖c(x) - l‖ where l is nlp.meta.lcon. Given xₖ, finds min ‖cₖ - l + Jₖd‖\n\n\n\n\n\n","category":"method"},{"location":"reference/#FletcherPenaltyNLPSolver.fps_solve-Union{Tuple{NLPModels.AbstractNLPModel}, Tuple{T}, Tuple{NLPModels.AbstractNLPModel, AbstractVector{T}}} where T","page":"Reference","title":"FletcherPenaltyNLPSolver.fps_solve","text":"Solver for equality constrained non-linear programs based on Fletcher's penalty function.\n\nCite: Estrin, R., Friedlander, M. P., Orban, D., & Saunders, M. A. (2020).\nImplementing a smooth exact penalty function for equality-constrained nonlinear optimization.\nSIAM Journal on Scientific Computing, 42(3), A1809-A1835.\n\nfps_solve(:: NLPStopping, :: AbstractVector{T};  σ_0 :: Number = one(T), σ_max :: Number = 1/eps(T), σ_update :: Number = T(1.15), unconstrained_solver :: Function = lbfgs) where T <: AbstractFloat or fps_solve(:: AbstractNLPModel, :: AbstractVector{T}, σ_0 :: Number = one(T), σ_max :: Number = 1/eps(T), σ_update :: Number = T(1.15), unconstrained_solver :: Function = lbfgs) where T <: AbstractFloat\n\nNotes:     \n\nIf the problem has inequalities, we use slack variables to get only equalities and bounds.\nstp.current_state.res contains the gradient of Fletcher's penalty function.\nunconstrained_solver must take an NLPStopping as input.\n\nTODO:\n\nune façon robuste de mettre à jour le paramètre de pénalité. [Convergence to infeasible stationary points]\nExtend to bounds and inequality constraints.\nHandle the tol_check from the paper !\nContinue to explore the paper.\n[Long term] Complemetarity constraints\n\n\n\n\n\n","category":"method"},{"location":"#FletcherPenaltyNLPSolver-Fletcher's-Penalty-Method","page":"Home","title":"FletcherPenaltyNLPSolver - Fletcher's Penalty Method","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"[![docs-stable][docs-stable-img]][docs-stable-url] [![docs-dev][docs-dev-img]][docs-dev-url] [![build-ci][build-ci-img]][build-ci-url] [![codecov][codecov-img]][codecov-url] [![release][release-img]][release-url] [![doi][doi-img]][doi-url]","category":"page"},{"location":"","page":"Home","title":"Home","text":"[docs-stable-img]: https://img.shields.io/badge/docs-stable-blue.svg [docs-stable-url]: https://tmigot.github.io/FletcherPenaltyNLPSolver/stable [docs-dev-img]: https://img.shields.io/badge/docs-dev-purple.svg [docs-dev-url]: https://tmigot.github.io/FletcherPenaltyNLPSolver/dev [build-ci-img]: https://github.com/tmigot/FletcherPenaltyNLPSolver/workflows/CI/badge.svg?branch=main [build-ci-url]: https://github.com/tmigot/FletcherPenaltyNLPSolver/actions [codecov-img]: https://codecov.io/gh/tmigot/FletcherPenaltyNLPSolver/branch/main/graph/badge.svg [codecov-url]: https://codecov.io/gh/tmigot/FletcherPenaltyNLPSolver [release-img]: https://img.shields.io/github/v/release/tmigot/FletcherPenaltyNLPSolver.svg?style=flat-square [release-url]: https://github.com/tmigot/FletcherPenaltyNLPSolver/releases [doi-img]: https://joss.theoj.org/papers/10.21105/joss.03991/status.svg [doi-url]: https://doi.org/10.21105/joss.03991","category":"page"},{"location":"","page":"Home","title":"Home","text":"FPS is a solver for equality-constrained nonlinear problems, i.e., optimization problems of the form","category":"page"},{"location":"","page":"Home","title":"Home","text":"min f(x)     s.t.     c(x) = 0,  ℓ ≤ x ≤ u.","category":"page"},{"location":"","page":"Home","title":"Home","text":"It uses other JuliaSmoothOptimizers packages for development. In particular, NLPModels.jl is used for defining the problem, and SolverCore for the output. If a general inequality-constrained problem is given to the solver, it solves the problem reformulated as a SlackModel from NLPModelsModifiers.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"We refer to juliasmoothoptimizers.github.io for tutorials on the NLPModel API. This framework allows the usage of models from Ampl (using AmplNLReader.jl), CUTEst (using CUTEst.jl), JuMP (using NLPModelsJuMP.jl), PDE-constrained optimization problems (using PDENLPModels.jl) and models defined with automatic differentiation (using ADNLPModels.jl).","category":"page"},{"location":"#Algorithm","page":"Home","title":"Algorithm","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The function fps_solver solves a nonlinear optimization problem by iteratively solving  the bound-constrained optimization problem using Fletcher penalty function:","category":"page"},{"location":"","page":"Home","title":"Home","text":"         beginaligned\n         min_x   f(x) - c(x)^T lambda_delta(x) + fracrho2c(x)^2_2 \n         mboxwhere  lambda_delta(x) in argmin_y frac12 nabla c(x)^T y - nabla f(x) ^2_2 + sigma c(x)^T y + fracdelta2y^2\n         endaligned","category":"page"},{"location":"","page":"Home","title":"Home","text":"For equality-constrained problems, the method iteratively solves an unconstrained problem. For bound and equality-constrained problems, the subproblems are bound-constrained problems. Any solver compatible with Stopping can be used. By default, we use ipopt from NLPModelsIpopt.jl to solve the subproblem, but other solvers can be used such as knitro from NLPModelsKnitro.jl or any solvers from JSOSolvers.jl. The Stopping version of these solvers is available in StoppingInterface.","category":"page"},{"location":"","page":"Home","title":"Home","text":"It uses LDLFactorizations.jl by default to evaluate the derivatives of the penalized subproblem, but one can also use a matrix-free version with Krylov.jl.","category":"page"},{"location":"#References","page":"Home","title":"References","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Estrin, R., Friedlander, M. P., Orban, D., & Saunders, M. A. (2020). Implementing a smooth exact penalty function for equality-constrained nonlinear optimization. SIAM Journal on Scientific Computing, 42(3), A1809-A1835. 10.1137/19M1238265","category":"page"},{"location":"#How-to-Cite","page":"Home","title":"How to Cite","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you use FletcherPenaltyNLPSolver in your work, please cite using the format given in CITATION.bib.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"pkg> add https://github.com/tmigot/FletcherPenaltyNLPSolver","category":"page"},{"location":"#Example","page":"Home","title":"Example","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We consider in this example the minization of the Rosenbrock function.","category":"page"},{"location":"","page":"Home","title":"Home","text":"    min_x  100 * (x₂ - x₁²)² + (x₁ - 1)²","category":"page"},{"location":"","page":"Home","title":"Home","text":"The problem is modeled using ADNLPModels.jl with [-1.2; 1.0] as default initial point, and then solved using fps_solve.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using FletcherPenaltyNLPSolver, ADNLPModels\n\n# Rosenbrock\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0])\nstats = fps_solve(nlp)","category":"page"},{"location":"","page":"Home","title":"Home","text":"We consider in this example the minization of the Rosenbrock function over an inequality constraint.","category":"page"},{"location":"","page":"Home","title":"Home","text":"    min_x  100 * (x₂ - x₁²)² + (x₁ - 1)² quad textst quad  0  x₁x₂  1","category":"page"},{"location":"","page":"Home","title":"Home","text":"The problem is modeled using ADNLPModels.jl with [-1.2; 1.0] as default initial point, and then solved using fps_solve.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using FletcherPenaltyNLPSolver, ADNLPModels\n\nnlp = ADNLPModel(\n  x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2,\n  [-1.2; 1.0],\n  x->[x[1] * x[2]],\n  [0.0],\n  [1.0],\n)\nstats = fps_solve(nlp)","category":"page"},{"location":"#Bug-reports-and-discussions","page":"Home","title":"Bug reports and discussions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you think you found a bug, feel free to open an issue. Focused suggestions and requests can also be opened as issues. Before opening a pull request, start an issue or a discussion on the topic, please.","category":"page"},{"location":"","page":"Home","title":"Home","text":"If you want to ask a question not suited for a bug report, feel free to start a discussion here. This forum is for general discussion about this repository and the JuliaSmoothOptimizers, so questions about any of our packages are welcome.","category":"page"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Pages = [\"tutorial.md\"]","category":"page"},{"location":"tutorial/#FletcherPenaltyNLPSolver-Tutorial","page":"Tutorial","title":"FletcherPenaltyNLPSolver Tutorial","text":"","category":"section"}]
}
