var documenterSearchIndex = {"docs":
[{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Contents","page":"Reference","title":"Contents","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Index","page":"Reference","title":"Index","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [FletcherPenaltyNLPSolver]","category":"page"},{"location":"reference/#FletcherPenaltyNLPSolver.FletcherPenaltyNLP","page":"Reference","title":"FletcherPenaltyNLPSolver.FletcherPenaltyNLP","text":"We consider here the implementation of Fletcher's exact penalty method for the minimization problem:\n\nminₓ f(x) s.t. c(x) = 0\n\nusing Fletcher penalty function:\n\nminₓ f(x) - dot(c(x),ys(x)) + ρ/2 dot(c(x),c(x))\n\nwhere\n\nys(x) := argmin\\_y 0.5 ||A(x)y - g(x)||²₂ + σ c(x)^T y + 0.5 δ ||²₂\n\nand denote Ys the gradient of ys(x).\n\nFletcherPenaltyNLP(:: AbstractNLPModel, :: Number, :: Function) or FletcherPenaltyNLP(:: AbstractNLPModel; σ_0 :: Real = 1.0)\n\nNotes:\n\nEvaluation of the obj, grad, objgrad functions evaluate functions from the orginial nlp.\n\nThese values are stored in fx, cx, gx.\n\nThe value of the penalty vector ys is also stored.\n\nTODO:\n\nsparse structure of the hessian?\n\nExample: fpsos  = FletcherPenaltyNLP(nlp, 0.1, _solvewithlinearoperator)\n\n\n\n\n\n","category":"type"},{"location":"reference/#FletcherPenaltyNLPSolver.feasibility_step-Union{Tuple{S}, Tuple{T}, Tuple{FletcherPenaltyNLPSolver.GNSolver, NLPModels.AbstractNLPModel, AbstractVector{T}, AbstractVector{T}, T, Union{AbstractMatrix{T}, LinearOperators.LinearOperator{T}}, T, AbstractFloat}} where {T, S}","page":"Reference","title":"FletcherPenaltyNLPSolver.feasibility_step","text":"feasibility_step(nls, x, cx, Jx)\n\nApproximately solves min ‖c(x)‖.\n\nGiven xₖ, finds min ‖cₖ + Jₖd‖\n\n\n\n\n\n","category":"method"},{"location":"reference/#FletcherPenaltyNLPSolver.fps_solve-Union{Tuple{NLPModels.AbstractNLPModel}, Tuple{T}, Tuple{NLPModels.AbstractNLPModel, AbstractVector{T}}} where T","page":"Reference","title":"FletcherPenaltyNLPSolver.fps_solve","text":"Solver for equality constrained non-linear programs based on Fletcher's penalty function.\n\nCite: Estrin, R., Friedlander, M. P., Orban, D., & Saunders, M. A. (2020).\nImplementing a smooth exact penalty function for equality-constrained nonlinear optimization.\nSIAM Journal on Scientific Computing, 42(3), A1809-A1835.\n\nfps_solve(:: NLPStopping, :: AbstractVector{T};  σ_0 :: Number = one(T), σ_max :: Number = 1/eps(T), σ_update :: Number = T(1.15), unconstrained_solver :: Function = lbfgs) where T <: AbstractFloat or fps_solve(:: AbstractNLPModel, :: AbstractVector{T}, σ_0 :: Number = one(T), σ_max :: Number = 1/eps(T), σ_update :: Number = T(1.15), unconstrained_solver :: Function = lbfgs) where T <: AbstractFloat\n\nNotes:     \n\nIf the problem has inequalities, we use slack variables to get only equalities and bounds.\nstp.current_state.res contains the gradient of Fletcher's penalty function.\nunconstrained_solver must take an NLPStopping as input.\n\nTODO:\n\nune façon robuste de mettre à jour le paramètre de pénalité. [Convergence to infeasible stationary points]\nExtend to bounds and inequality constraints.\nHandle the tol_check from the paper !\nContinue to explore the paper.\n[Long term] Complemetarity constraints\n\n\n\n\n\n","category":"method"},{"location":"reference/#FletcherPenaltyNLPSolver.status_stopping_to_stats-Tuple{Stopping.AbstractStopping}","page":"Reference","title":"FletcherPenaltyNLPSolver.status_stopping_to_stats","text":"Return the status in GenericStats from a Stopping.\n\n\n\n\n\n","category":"method"},{"location":"reference/#FletcherPenaltyNLPSolver.stopping_to_stats-Tuple{Stopping.NLPStopping}","page":"Reference","title":"FletcherPenaltyNLPSolver.stopping_to_stats","text":"Initialize a GenericStats from Stopping\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSolvers.lbfgs-Union{Tuple{T}, Tuple{FletcherPenaltyNLP, AbstractVector{T}}} where T<:AbstractFloat","page":"Reference","title":"JSOSolvers.lbfgs","text":"Adaptation from lbfgs of JSOSolvers.jl https://github.com/JuliaSmoothOptimizers/JSOSolvers.jl/blob/master/src/lbfgs.jl\n\nhandle objgrad whenever possible\nStopping buffer\nadapt output\nremove stats and info-log (maybe put it back)\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModelsIpopt.ipopt-Tuple{Stopping.NLPStopping}","page":"Reference","title":"NLPModelsIpopt.ipopt","text":"ipopt(nlp) DOESN'T CHECK THE WRONG KWARGS, AND RETURN AN ERROR. ipopt(::NLPStopping)\n\n\n\n\n\n","category":"method"},{"location":"#FletcherPenaltyNLPSolver","page":"Home","title":"FletcherPenaltyNLPSolver","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: CI) (Image: codecov)","category":"page"},{"location":"","page":"Home","title":"Home","text":"This implementation uses Stopping.","category":"page"},{"location":"#Algorithm","page":"Home","title":"Algorithm","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The function Fletcher_penalty_solver(nlp :: AbstractNLPModel) solves a nonlinear optimization problem with equality constraints by iteratively solving  the unconstrained optimization problem using Fletcher penalty function:","category":"page"},{"location":"","page":"Home","title":"Home","text":"         beginaligned\n         min_x   f(x) - c(x)^T lambda_delta(x) + fracrho2c(x)^2_2 \n         mboxwhere  lambda_delta(x) in argmin_y frac12 nabla c(x)^T y - nabla f(x) ^2_2 + sigma c(x)^T y + fracdelta2y^2\n         endaligned","category":"page"},{"location":"#References","page":"Home","title":"References","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Estrin, R., Friedlander, M. P., Orban, D., & Saunders, M. A. (2020).   Implementing a smooth exact penalty function for equality-constrained nonlinear optimization.   SIAM Journal on Scientific Computing, 42(3), A1809-A1835.","category":"page"},{"location":"#Versions-[On-hold]","page":"Home","title":"Versions [On hold]","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package still on unofficial versions of:","category":"page"},{"location":"","page":"Home","title":"Home","text":"ADNLPModels v0.1.1 https://github.com/tmigot/ADNLPModels.jl#exlude-jth_functions    \nNLPModelsKnitro v0.5.0 https://github.com/tmigot/NLPModelsKnitro.jl#solvertools050","category":"page"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Pages = [\"tutorial.md\"]","category":"page"},{"location":"tutorial/#FletcherPenaltyNLPSolver-Tutorial","page":"Tutorial","title":"FletcherPenaltyNLPSolver Tutorial","text":"","category":"section"}]
}
